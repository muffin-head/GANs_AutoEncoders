{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOAYuoTwHEkmoChwCpdmnPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muffin-head/GANs_AutoEncoders/blob/main/tensorflow_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "8vBrbGgd5CLv",
        "outputId": "02cfada2-9f23-44eb-aeb7-4159ae4745dc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.examples'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-ac9c0cf52aea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from tensorflow.examples.tutorials.mnist.input_data import input_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "Di55YjpPQBhk"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1], flatten them, and ensure dtype is tf.float32\n",
        "train_images = tf.cast(train_images.reshape(-1, 784) / 255.0, tf.float32)\n",
        "train_labels = tf.one_hot(train_labels, depth=10)\n",
        "\n",
        "# Proceed with the rest of your code...\n"
      ],
      "metadata": {
        "id": "WTBPCyk3REeX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images,test_images=train_images/255.0,test_images/255.0"
      ],
      "metadata": {
        "id": "AmrevUT2RIRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pJYf_quwR0XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape[0],test_images.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g7HwG2bR7Hr",
        "outputId": "344ecf9a-105d-4a6d-ad3f-f68268d87cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape[1]*train_images.shape[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs4AlDYRSBQu",
        "outputId": "f3758838-869e-4375-cc7c-2bc9edced2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(np.unique(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPMRF_9pST4j",
        "outputId": "296d7cea-e353-4d8b-c0e0-79fa05ec646c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "VxNtCDuISlFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSC3JzRaTcKN",
        "outputId": "7ee4ecab-966f-44df-d371-162315044fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 12707095218897085134\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14626652160\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 11307089556464436990\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=tf.keras.Input(shape=(None,784))"
      ],
      "metadata": {
        "id": "RFT_OVkNT0fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=tf.Variable(tf.zeros([784,10]))\n",
        "b=tf.Variable(tf.zeros([10]))"
      ],
      "metadata": {
        "id": "SFDJ_oTjYA1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=tf.keras.layers.Softmax()(tf.matmul(x,w)+b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxu-C9qjYvmm",
        "outputId": "5eab3de9-a372-429b-8077-aecce985a2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_2), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(784, 10) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_2), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(10,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_=tf.keras.Input(shape=(None,10))"
      ],
      "metadata": {
        "id": "j3BWqC7kZh9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy=tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(y),1))"
      ],
      "metadata": {
        "id": "jMqL2-XFZrDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1], flatten them, and ensure dtype is tf.float32\n",
        "train_images = tf.cast(train_images.reshape(-1, 784) / 255.0, tf.float32)\n",
        "train_labels = tf.one_hot(train_labels, depth=10)\n",
        "\n",
        "# Proceed with the rest of your code...\n",
        "\n",
        "# Model function\n",
        "def model(x):\n",
        "\treturn tf.keras.layers.Softmax()(tf.matmul(x,w)+b)\n",
        "\n",
        "# Cross-entropy loss function\n",
        "def cross_entropy(y_true, y_pred):\n",
        "\treturn tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
        "\n",
        "# Initialize the SGD optimizer\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.05)\n",
        "\n",
        "# Example: Calculate loss and apply gradients for the first batch of 32 images\n",
        "batch_size = 32\n",
        "x_batch = train_images[:batch_size]\n",
        "y_true_batch = train_labels[:batch_size]\n",
        "\n",
        "# Compute the predictions and loss using tf.GradientTape for automatic differentiation\n",
        "with tf.GradientTape() as tape:\n",
        "\ty_pred_batch = model(x_batch)\n",
        "\tloss = cross_entropy(y_true_batch, y_pred_batch)\n",
        "\n",
        "# Calculate gradients of loss with respect to the model variables\n",
        "gradients = tape.gradient(loss, [w, b])\n",
        "print(\"W before update:\", w.numpy()[:5])  # Print the first 5 elements for brevity\n",
        "print(\"b before update:\", b.numpy())\n",
        "\n",
        "optimizer.apply_gradients(zip(gradients, [w, b]))\n",
        "\n",
        "print(\"W after update:\", w.numpy()[:5])  # Again, just the first 5 elements\n",
        "print(\"b after update:\", b.numpy())\n",
        "\n",
        "# Apply the gradients to the optimizer\n",
        "\n",
        "print(\"Cross-entropy loss for the first batch after applying gradients:\", loss.numpy())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIM7idk1cFpe",
        "outputId": "8cc66533-1639-4eaa-b0b2-ad16996a2b6a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W before update: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "b before update: [-0.00374062  0.00872809  0.00249376  0.00561093  0.00249376 -0.00374062\n",
            " -0.00374062 -0.00374062 -0.00374062 -0.00062343]\n",
            "W after update: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "b after update: [-0.00559691  0.01305931  0.00373132  0.00839535  0.00373132 -0.00559691\n",
            " -0.00559691 -0.00559691 -0.00559691 -0.00093276]\n",
            "Cross-entropy loss for the first batch after applying gradients: 2.3006854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1], flatten them, and ensure dtype is tf.float32\n",
        "train_images = tf.cast(train_images.reshape(-1, 784) / 255.0, tf.float32)\n",
        "train_labels = tf.one_hot(train_labels, depth=10)\n",
        "\n",
        "# Variables for model parameters\n",
        "w = tf.Variable(tf.random.normal([784, 10], stddev=0.01))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Model function\n",
        "def model(x):\n",
        "    return tf.keras.layers.Softmax()(tf.matmul(x, w) + b)\n",
        "\n",
        "# Cross-entropy loss function\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), axis=[1]))\n",
        "\n",
        "# Initialize the SGD optimizer\n",
        "optimizer = tf.optimizers.SGD(learning_rate=0.05)\n",
        "\n",
        "# Number of epochs and batch size\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "steps_per_epoch = len(train_images) // batch_size\n",
        "\n",
        "# Training loop for multiple epochs\n",
        "for epoch in range(epochs):\n",
        "    for step in range(steps_per_epoch):\n",
        "        # Slice a batch of data\n",
        "        batch_start = step * batch_size\n",
        "        batch_end = batch_start + batch_size\n",
        "        x_batch = train_images[batch_start:batch_end]\n",
        "        y_true_batch = train_labels[batch_start:batch_end]\n",
        "\n",
        "        # Compute predictions and loss within a GradientTape context\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred_batch = model(x_batch)\n",
        "            loss = cross_entropy(y_true_batch, y_pred_batch)\n",
        "\n",
        "        # Calculate gradients and update model parameters\n",
        "        gradients = tape.gradient(loss, [w, b])\n",
        "        optimizer.apply_gradients(zip(gradients, [w, b]))\n",
        "\n",
        "    # Optional: Print loss at the end of each epoch\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb-Ds3s9f3OA",
        "outputId": "a54bf671-68e2-4b13-80f1-504c44ae77ce"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x79deec8beb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x79deec8beb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.1775708794593811\n",
            "Epoch 2, Loss: 0.14608615636825562\n",
            "Epoch 3, Loss: 0.13153254985809326\n",
            "Epoch 4, Loss: 0.12244536727666855\n",
            "Epoch 5, Loss: 0.11608962714672089\n",
            "Epoch 6, Loss: 0.11136554926633835\n",
            "Epoch 7, Loss: 0.1077132523059845\n",
            "Epoch 8, Loss: 0.10480792820453644\n",
            "Epoch 9, Loss: 0.10244493186473846\n",
            "Epoch 10, Loss: 0.10048765689134598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to [0, 1], flatten them, and ensure dtype is tf.float32\n",
        "train_images = tf.cast(train_images.reshape(-1, 784) / 255.0, tf.float32)\n",
        "train_labels = tf.one_hot(train_labels, depth=10)\n",
        "\n",
        "# Do the same preprocessing for test images and labels\n",
        "test_images = tf.cast(test_images.reshape(-1, 784) / 255.0, tf.float32)\n",
        "test_labels = tf.one_hot(test_labels, depth=10)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "plt.ion()\n",
        "\n",
        "\n",
        "n_hidden_layer1=512\n",
        "n_hidden_layer2=128\n",
        "n_classes=10\n",
        "input_shape=784\n",
        "\n",
        "\n",
        "# Your dataset loading and preprocessing code remains unchanged\n",
        "\n",
        "# Fixed weights and biases initialization\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random.normal([input_shape, n_hidden_layer1], stddev=0.039, mean=0)),\n",
        "    'h2': tf.Variable(tf.random.normal([n_hidden_layer1, n_hidden_layer2], stddev=0.055, mean=0)),\n",
        "    'out': tf.Variable(tf.random.normal([n_hidden_layer2, n_classes], stddev=0.120, mean=0))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random.normal([n_hidden_layer1])),\n",
        "    'b2': tf.Variable(tf.random.normal([n_hidden_layer2])),\n",
        "    'out': tf.Variable(tf.random.normal([n_classes]))\n",
        "}\n",
        "\n",
        "# Fixed MLP model function\n",
        "def mlp(x, weights, biases):\n",
        "    layer1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer1 = tf.nn.sigmoid(layer1)\n",
        "\n",
        "    layer2 = tf.add(tf.matmul(layer1, weights['h2']), biases['b2'])\n",
        "    layer2 = tf.nn.sigmoid(layer2)\n",
        "\n",
        "    output_layer = tf.add(tf.matmul(layer2, weights['out']), biases['out'])\n",
        "    output_layer = tf.nn.sigmoid(output_layer)\n",
        "\n",
        "    return output_layer\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.05)\n",
        "# Updated training loop with dynamic plotting\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Fixed dynamic plotting function\n",
        "def dyn_plot(x, y, y_test, ax, ticks, title):\n",
        "    ax.clear()\n",
        "    ax.plot(x, y, 'b', label=\"Train Loss\")\n",
        "    ax.plot(x, y_test, 'r', label=\"Test Loss\")\n",
        "    plt.xticks(ticks)\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    fig.canvas.draw()\n",
        "\n",
        "# Train and evaluate the model\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for step in range(steps_per_epoch):\n",
        "        batch_start = step * batch_size\n",
        "        batch_end = batch_start + batch_size\n",
        "        x_batch = train_images[batch_start:batch_end]\n",
        "        y_true_batch = train_labels[batch_start:batch_end]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred_batch = mlp(x_batch, weights, biases)\n",
        "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_batch, labels=y_true_batch))\n",
        "            epoch_loss += loss.numpy()\n",
        "\n",
        "        gradients = tape.gradient(loss, list(weights.values()) + list(biases.values()))\n",
        "        optimizer.apply_gradients(zip(gradients, list(weights.values()) + list(biases.values())))\n",
        "\n",
        "    train_losses.append(epoch_loss / steps_per_epoch)\n",
        "\n",
        "    # Test loss calculation\n",
        "    y_pred_test = mlp(test_images, weights, biases)\n",
        "    test_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_test, labels=test_labels))\n",
        "    test_losses.append(test_loss.numpy())\n",
        "\n",
        "    # Dynamic plot update\n",
        "    dyn_plot(list(range(1, epoch + 2)), train_losses, test_losses, ax, np.arange(1, epoch + 3), \"Training and Test Loss\")\n",
        "\n",
        "plt.ioff()  # Turn off interactive mode\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EVHRFimRgGbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edp_Jjum_lTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}