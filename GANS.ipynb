{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNptdLvrFZGzWY9jJFhB8Ll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muffin-head/GANs_AutoEncoders/blob/main/GANS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ng74W-us_Emp"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, Dense, Lambda # lamda is used for custom operations\n",
        "from keras.models import Model\n",
        "from keras import backend as k # backend is more connecting keras to theano or TF functionality\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2DSygCS_NjR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  Orignal image dim is 28x28, so flatten it i.e 784 (28x28) we have single intermediate layer as well we can have value, lets say 256(its hyper param)"
      ],
      "metadata": {
        "id": "MFYvN6RgJHEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 100\n",
        "orignal_dim= 28*28\n",
        "latent_dim= 2     # compression of 28x28 to 2x2 dimension from which reconstruction will happen, its final o/p of encoder\n",
        "Intermediate_dim= 256  #layer in which the input image i.e 28x28 i.e 784 is compressed and then passed to latent\n",
        "nb_epoch=5\n",
        "epsilon_std=1.0 # noise to be added\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "06bhAi6NDTTf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) k.shape(z_mean)[0] --> how dense layer works is its: Its the weighted sum of the input, lets assume the dense layer is 348 and next dense layer is 2(latent one) so it will do the weighted sum of it and pass it and so the dim will be [weighted sum, 2] same for another remaining neuron as well, so to main the shape in sampling we use shape that takes 1st object i.e the batch size of vector and latent dim.\n",
        "\n",
        "k.exp(z_log_var/2) * epsilon --> This is nothing but the noise and adding mean to it\n",
        "\n",
        "\n",
        "Basically we sample frokm this districution and feed these values to the decoder.\n"
      ],
      "metadata": {
        "id": "K-xxwbQhSkbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(args:tuple):\n",
        "  z_mean,z_log_var=args\n",
        "  epsilon=k.random_normal(shape=(k.shape(z_mean)[0],latent_dim),mean=0,stddev=epsilon_std)\n",
        "  return z_mean+ k.exp(z_log_var/2) * epsilon"
      ],
      "metadata": {
        "id": "KHrDpUmvRFU9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= Input(shape=(orignal_dim,),name='input')\n",
        "h=Dense(Intermediate_dim,activation='relu', name='encoding')(x)  #activation func: when the neurons to be activated, relu=if no. is pos= as it is , if no. neg= turns it to 0\n",
        "z_mean = Dense(latent_dim, name='mean')(h) #Defines the mean of the latent space\n",
        "z_log_var=Dense(latent_dim,name='log_var')(h) #defines the var of the latent space\n",
        "# now i have mean and var layer above right!, I will take this both and put as input to lambda func, now sampling is the lambda function which will take random samples from the normal dist (from mean and var that we passed and give us output in dim of latemt dim itself)\n",
        "z= Lambda(sampling,output_shape=(latent_dim,))([z_mean,z_log_var])\n",
        "encoder=Model(x,[z_mean,z_log_var,z],name='encoder')"
      ],
      "metadata": {
        "id": "g9JgXemVHyOF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlMUmlR7MgA1",
        "outputId": "3f780682-85d3-4fcf-a694-f0ed0a9da014"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input (InputLayer)          [(None, 784)]                0         []                            \n",
            "                                                                                                  \n",
            " encoding (Dense)            (None, 256)                  200960    ['input[0][0]']               \n",
            "                                                                                                  \n",
            " mean (Dense)                (None, 2)                    514       ['encoding[0][0]']            \n",
            "                                                                                                  \n",
            " log_var (Dense)             (None, 2)                    514       ['encoding[0][0]']            \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)           (None, 2)                    0         ['mean[0][0]',                \n",
            "                                                                     'log_var[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 201988 (789.02 KB)\n",
            "Trainable params: 201988 (789.02 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EE-e03zKVNzp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}